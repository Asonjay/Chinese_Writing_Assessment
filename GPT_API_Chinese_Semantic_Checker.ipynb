{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GPT API Chinese Semantic Checker\n",
    "\n",
    "This notebook is used to check the semantic of Chinese language unit. It is based on GPT-3.5 and GPT-4 's API. \n",
    "\n",
    "Author: Zexin Xu, Zilu Zhang"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GPT API Query\n",
    "\n",
    "This snippet invloves the Baidu API. `API_key` is deleted for security reasons. Please use your own API key. For more details of the usage of GPT models, please refer to [GPT API](https://platform.openai.com/docs/introduction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import openai\n",
    "from datetime import datetime\n",
    "\n",
    "API_KEY = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "openai.api_key = API_KEY\n",
    "#NOTE If you don't havve access to GPT-4, feel free to use GPT-3.5-turbo or other suitiable models\n",
    "model_id = \"gpt-4\"\n",
    "\n",
    "def ChatGPT_conversation(conversation):\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=model_id,\n",
    "        messages=conversation\n",
    "    )\n",
    "    conversation.append({'role': response.choices[0].message.role, 'content': response.choices[0].message.content})\n",
    "    return conversation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#NOTE We are asking in the context of Chinese\n",
    "question_type = ['语病']\n",
    "question_suffix = '，这句话是否有'\n",
    "\n",
    "#NOTE Put your own dataframe here, and change column names accordingly\n",
    "chatgpt_tunit_df = pd.DataFrame()\n",
    "data_df = chatgpt_tunit_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "\"\"\"\n",
    "    This setup is to solve GPT hour limit issue. Generating multiple files and combine them later.\n",
    "    If you received a bad gateway error or rate limit error, the code will stop and print out the index \n",
    "    of the last row. So you can start from that index and run the code again. Also remember to change the\n",
    "    file output path name. The second try-except function is to prevent overwriting existing file accidentally.\n",
    "\"\"\"\n",
    "\n",
    "start_index = 0\n",
    "stop_index = 0\n",
    "conversation = []\n",
    "try:\n",
    "    for i, row in data_df.loc[start_index:, :].iterrows():\n",
    "        for q_type in question_type:\n",
    "            question = '“' + row['sentence'] + \"”\" + question_suffix + q_type + \"？\"\n",
    "            # Append is to include previous interaction\n",
    "            conversation.append({'role': 'user', 'content': question})\n",
    "            conversation = ChatGPT_conversation(conversation)\n",
    "            data_df.loc[i, q_type] = conversation[-1]['content'].strip()\n",
    "            if row['sen'] == 1:\n",
    "                conversation = []\n",
    "        stop_index = i\n",
    "        if i % 20 == 0:\n",
    "            print(f\"{i}th iteration done...\")\n",
    "except Exception as e:\n",
    "    print(type(e).__name__, \"/\", str(e))\n",
    "    print(\"stop at index: \", stop_index - 1)\n",
    "    now = datetime.now()\n",
    "    print(\"current time = \", now.strftime(\"%H:%M:%S\"))\n",
    "\n",
    "#NOTE Prevent overwriting existing file\n",
    "try:\n",
    "    data_df.to_csv('chatgpt/GPT4/yubing/tunit_df_result_context.csv', mode='x', index=False, encoding='utf-8-sig')\n",
    "except FileExistsError:\n",
    "    print('File already exists! Change it to another name.')\n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Result processsing\n",
    "\n",
    "This snippet includes the result processing of the GPT API. We use keyword filtering to determine the label of returned result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "\n",
    "result_tunit_df = pd.read_csv('chatgpt/GPT4/yubing/tunit_df_result_context.csv', encoding='utf-8-sig')\n",
    "result_sen_df = pd.read_csv('chatgpt/GPT4/yubing/sen_df_result.csv', encoding='utf-8-sig')\n",
    "\n",
    "def modify_yufa_result(df):\n",
    "    for i, row in df.iterrows():\n",
    "        if ('没有语病' in row['语病'] or\n",
    "            '没有语病错误' in row['语病'] or\n",
    "            '不算有语病' in row['语病'] or\n",
    "            '没有问题' in row['语病'] or\n",
    "            '语病正确' in row['语病'] or\n",
    "            '语病上是正确的' in row['语病'] or\n",
    "            '语病上没有错误' in row['语病'] or\n",
    "            '语病没有错误' in row['语病'] or\n",
    "            '没有语病问题' in row['语病'] or\n",
    "            '没有明显错误' in row['语病'] or\n",
    "            '不算是语病' in row['语病'] or \n",
    "            '语法上可以说是正确的' in row['语病'] or\n",
    "            '语法上没有明显错误' in row['语病'] or \n",
    "            '没有明显的' in row['语病'] or \n",
    "            '没有显著的' in row['语病'] or\n",
    "            '没有错误' in row['语病'] or\n",
    "            '完全正确' in row['语病'] or\n",
    "            '基本正确' in row['语病'] or    \n",
    "            '基本上正确' in row['语病'] or \n",
    "            '是正确的语法' in row['语病'] or\n",
    "            '标准的英语表达' in row['语病'] or \n",
    "            '语法是正确的' in row['语病'] or\n",
    "            '语法正确' in row['语病'] or\n",
    "            '没有语法错误' in row['语病'] or\n",
    "            '没有。' in row['语病']):\n",
    "            df.loc[i, 'yubing_label'] = 1\n",
    "        else:\n",
    "            df.loc[i, 'yubing_label'] = 0\n",
    "\n",
    "modify_yufa_result(result_tunit_df)\n",
    "modify_yufa_result(result_sen_df)\n",
    "result_tunit_df.to_csv('chatgpt/GPT4/tunit_df_result_mod.csv', index=False, encoding='utf-8-sig')\n",
    "result_sen_df.to_csv('chatgpt/GPT4/sen_df_result_mod.csv', index=False, encoding='utf-8-sig')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "75273f6ed8af91899ebc591bf6ae2fd0716c5db2515d7097a000123632f4e53a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
