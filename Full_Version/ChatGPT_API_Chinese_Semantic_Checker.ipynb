{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ChatGPT API Chinese Semantic Checker\n",
    "\n",
    "Author: Zexin Xu, Zilu Zhang"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing\n",
    "\n",
    "For this dataset only. Do not run this for other datasets.\n",
    "\n",
    "* `tunit_df` includes tunits data\n",
    "* `sen_df` includes sentences data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel('mysen_edit.xlsx')  # read excel file\n",
    "df.dropna(subset=['sentences', 'correct_final'], inplace=True)\n",
    "df = df.reset_index(drop=True)  # drop empty row \n",
    "df['sentences'] = df['sentences'].str.replace(r'_x000D_\\n', '', regex=True).replace(r'\\n', '', regex=True) # remove _x000D_\\n\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Added new correct label\n",
    "new_truth_df = pd.read_csv(\"ForGPT4.csv\")\n",
    "\n",
    "tunit_df = pd.DataFrame({\n",
    "    'sentence': df['sentences'], \n",
    "    'ground_truth_label': new_truth_df['ground_truth_label0306'],\n",
    "    'sen': df['sen']\n",
    "})\n",
    "tunit_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent = \"\"\n",
    "correct = True\n",
    "sent_arr = []\n",
    "correct_arr = []\n",
    "for i, row in df.iterrows():\n",
    "    sent += row['sentences']\n",
    "    # correct = correct and row['correct_final']\n",
    "    if row['sen'] == 0:\n",
    "        sent += \"，\"\n",
    "    else:\n",
    "        sent += \"。\"\n",
    "        sent.replace(\"_x000D_\\n\", \"\")\n",
    "        sent_arr.append(sent)\n",
    "        # correct_arr.append(correct)\n",
    "        # reset\n",
    "        sent = \"\"\n",
    "        # correct = True\n",
    "\n",
    "# Add new ground truth label df\n",
    "new_truth_sen_df = pd.read_csv(\"ForGPT4_sen.csv\")\n",
    "\n",
    "sen_df = pd.DataFrame({\n",
    "    'sentence': sent_arr, \n",
    "    'ground_truth_label': new_truth_sen_df['ground_truth_label_0306']\n",
    "})\n",
    "sen_df.head()   "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ChatGPT API Query\n",
    "\n",
    "Source: \n",
    "- https://platform.openai.com/docs/api-reference/authentication\n",
    "- https://learndataanalysis.org/getting-started-with-openai-gpt-gpt-3-5-model-api-in-python/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "API_KEY = \"\"\n",
    "\n",
    "openai.api_key = API_KEY\n",
    "model_id = \"gpt-4\"\n",
    "\n",
    "def ChatGPT_conversation(conversation):\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=model_id,\n",
    "        messages=conversation\n",
    "    )\n",
    "    conversation.append({'role': response.choices[0].message.role, 'content': response.choices[0].message.content})\n",
    "    return conversation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Query Logic\n",
    "\n",
    "Using different question to check whether a sentence is grammatically correct.\n",
    "1. `语病`: an overall check of the sentence\n",
    "2. `拼写错误`: a check whether the sentence contains spelling errors\n",
    "3. `语法错误`: a check whether the sentence contains grammatical errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "# question_type = ['语病', '字词错误', '语法错误'']\n",
    "question_type = ['语病']\n",
    "question_suffix = '，这句话是否有'\n",
    "\n",
    "chatgpt_tunit_df = tunit_df.copy()\n",
    "\n",
    "#NOTE This setup is to solve ChatGPT hour limit issue. Generating multiple files and combine them later.\n",
    "data_df = chatgpt_tunit_df\n",
    "start_index = 1933\n",
    "stop_index = 0\n",
    "conversation = []\n",
    "try:\n",
    "    for i, row in data_df.loc[start_index:, :].iterrows():\n",
    "        for q_type in question_type:\n",
    "            question = '“' + row['sentence'] + \"”\" + question_suffix + q_type + \"？\"\n",
    "            conversation.append({'role': 'user', 'content': question})\n",
    "            conversation = ChatGPT_conversation(conversation)\n",
    "            data_df.loc[i, q_type] = conversation[-1]['content'].strip()\n",
    "            if row['sen'] == 1:\n",
    "                conversation = []\n",
    "        stop_index = i\n",
    "        if i % 20 == 0:\n",
    "            print(f\"{i}th iteration done...\")\n",
    "except Exception as e:\n",
    "    print(type(e).__name__, \"/\", str(e))\n",
    "    print(\"stop at index: \", stop_index - 1)\n",
    "    now = datetime.now()\n",
    "    print(\"current time = \", now.strftime(\"%H:%M:%S\"))\n",
    "\n",
    "#NOTE Prevent overwriting existing file\n",
    "try:\n",
    "    data_df.to_csv('chatgpt/GPT4/yubing/tunit_df_result_context7.csv', mode='x', index=False, encoding='utf-8-sig')\n",
    "except FileExistsError:\n",
    "    print('File already exists! Change it to another name.')\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "# question_type = ['语病', '字词错误', '语法错误'']\n",
    "question_type = ['语病']\n",
    "question_suffix = '，这句话是否有'\n",
    "\n",
    "chatgpt_tunit_df = sen_df.copy()\n",
    "\n",
    "#NOTE This setup is to solve ChatGPT hour limit issue. Generating multiple files and combine them later.\n",
    "data_df = chatgpt_tunit_df\n",
    "start_index = 283\n",
    "stop_index = 0\n",
    "conversation = []\n",
    "try:\n",
    "    for i, row in data_df.loc[start_index:, :].iterrows():\n",
    "        for q_type in question_type:\n",
    "            question = '“' + row['sentence'] + \"”\" + question_suffix + q_type + \"？\"\n",
    "            conversation = [{'role': 'user', 'content': question}]\n",
    "            conversation = ChatGPT_conversation(conversation)\n",
    "            data_df.loc[i, q_type] = conversation[-1]['content'].strip()\n",
    "        stop_index = i\n",
    "        if i % 20 == 0:\n",
    "            print(f\"{i}th iteration done...\")\n",
    "except Exception as e:\n",
    "    print(type(e).__name__, \"/\", str(e))\n",
    "    print(\"stop at index: \", stop_index - 1)\n",
    "    now = datetime.now()\n",
    "    print(\"current time = \", now.strftime(\"%H:%M:%S\"))\n",
    "\n",
    "#NOTE Prevent overwriting existing file\n",
    "try:\n",
    "    data_df.to_csv('chatgpt/GPT4/yubing/sen_df_result1.csv', mode='x', index=False, encoding='utf-8-sig')\n",
    "except FileExistsError:\n",
    "    print('File already exists! Change it to another name.')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Time/Cost Result (GPT4)\n",
    "1. tunit dataset\n",
    "    - Time: \n",
    "        - 33min 11s\n",
    "        - 59min 46s\n",
    "        - 5min 24s\n",
    "        - 2h 46min 17s\n",
    "        - 20min 58.9s\n",
    "        - 6min 15s\n",
    "        - 58min 17s\n",
    "        - 41min 28s\n",
    "        - = 391min 39.6s = 6h 31m 39.6s\n",
    "    - Cost: \n",
    "        - 11.27$\n",
    "2. sen dataset\n",
    "    - Time:\n",
    "        - 47min 39s\n",
    "        - 2h 34min 12s\n",
    "        - = 201min 51s = 3h 31m 51s\n",
    "    - Cost:\n",
    "        - 5.78$\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Time/Cost Result\n",
    "1. tunit dataset\n",
    "    - 40m56s\n",
    "    - $0.47\n",
    "\n",
    "2. sen dataset\n",
    "    - 19m52s+ 8m31.4s = 27m23.4s\n",
    "    - $0.17\n",
    "\n",
    "3. modified tunit dataset\n",
    "    - 1min 52\n",
    "    - 2min 52\n",
    "    - 3min 1.2\n",
    "    - 10min 10s\n",
    "    - 7min 36s\n",
    "    - 1min 30s\n",
    "    - 2min 35s\n",
    "    - 4min 12s\n",
    "    - 2min 54s\n",
    "    - 8min 21\n",
    "    - 5min 49\n",
    "    - 1min 23\n",
    "    - 7min 53\n",
    "    - 4min 55s\n",
    "    - 4min 55s\n",
    "    - Total: 65min 3s\n",
    "    - $0.40"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Result processsing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################\n",
    "#NOTE Dataframe Processing\n",
    "###############################\n",
    "import pandas as pd\n",
    "\n",
    "result_tunit_df = pd.read_csv('chatgpt/GPT3.5-turbo/yubing/tunit_df_result.csv', encoding='utf-8-sig')\n",
    "result_tunit_context_df = pd.read_csv('chatgpt/GPT3.5-turbo/yubing/tunit_df_result_context.csv', encoding='utf-8-sig')\n",
    "result_sen_df = pd.read_csv('chatgpt/GPT3.5-turbo/yubing/sen_df_result.csv', encoding='utf-8-sig')\n",
    "\n",
    "\n",
    "###############################\n",
    "#NOTE 语病 result df processing\n",
    "###############################\n",
    "\n",
    "def modify_yubing_result(df):\n",
    "    for i, row in df.iterrows():\n",
    "        if row['语病'] == \"1\":\n",
    "            df.loc[i, 'yubing_label'] = 1\n",
    "        elif row['语病'] == \"0\":\n",
    "            df.loc[i, 'yubing_label'] = 0\n",
    "        else:\n",
    "            if ('没有语病' in row['语病'] or\n",
    "                '没有明显语病' in row['语病'] or\n",
    "                '无语病' in row['语病'] or\n",
    "                '不是' in row['语病'] or\n",
    "                '否' in row['语病'] or\n",
    "                '没有' in row['语病'] or\n",
    "                '没有语法错误' in row['语病'] or\n",
    "                '没有问题' in row['语病'] or\n",
    "                '语法正确' in row['语病'] or\n",
    "                '语法上是正确的' in row['语病'] or\n",
    "                '语法上没有错误' in row['语病'] or\n",
    "                '语法没有错误' in row['语病'] or\n",
    "                '没有语法问题' in row['语病'] or\n",
    "                '没有明显错误' in row['语病'] or\n",
    "                '不算是语病' in row['语病'] or \n",
    "                '语法上可以说是正确的' in row['语病'] or\n",
    "                '语法上没有明显错误' in row['语病'] or \n",
    "                '没有明显的' in row['语病'] or \n",
    "                '没有显著的' in row['语病'] or\n",
    "                '没有错误' in row['语病'] or\n",
    "                '完全正确' in row['语病'] or\n",
    "                '基本正确' in row['语病'] or \n",
    "                '是正确的语法' in row['语病'] or\n",
    "                '标准的英语表达' in row['语病'] ):\n",
    "                df.loc[i, 'yubing_label'] = 1\n",
    "            else:\n",
    "                df.loc[i, 'yubing_label'] = 0\n",
    "\n",
    "modify_yubing_result(result_tunit_df)\n",
    "modify_yubing_result(result_tunit_context_df)\n",
    "modify_yubing_result(result_sen_df)\n",
    "\n",
    "result_tunit_df['语病_context'] = result_tunit_context_df['语病']\n",
    "result_tunit_df['yubing_label_context'] = result_tunit_context_df['yubing_label']\n",
    "result_tunit_df.head()\n",
    "\n",
    "result_tunit_df.to_csv('chatgpt/GPT3.5-turbo/tunit_df_result_mod.csv', index=False, encoding='utf-8-sig')\n",
    "result_sen_df.to_csv('chatgpt/GPT3.5-turbo/sen_df_result_mod.csv', index=False, encoding='utf-8-sig')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################\n",
    "#NOTE 语病 result df processing GPT4\n",
    "###############################\n",
    "import pandas as pd \n",
    "\n",
    "result_tunit_df = pd.read_csv('chatgpt/GPT4/yubing/tunit_df_result_context.csv', encoding='utf-8-sig')\n",
    "result_sen_df = pd.read_csv('chatgpt/GPT4/yubing/sen_df_result.csv', encoding='utf-8-sig')\n",
    "\n",
    "def modify_yufa_result(df):\n",
    "    for i, row in df.iterrows():\n",
    "        if ('没有语病' in row['语病'] or\n",
    "            '没有语病错误' in row['语病'] or\n",
    "            '不算有语病' in row['语病'] or\n",
    "            '没有问题' in row['语病'] or\n",
    "            '语病正确' in row['语病'] or\n",
    "            '语病上是正确的' in row['语病'] or\n",
    "            '语病上没有错误' in row['语病'] or\n",
    "            '语病没有错误' in row['语病'] or\n",
    "            '没有语病问题' in row['语病'] or\n",
    "            '没有明显错误' in row['语病'] or\n",
    "            '不算是语病' in row['语病'] or \n",
    "            '语法上可以说是正确的' in row['语病'] or\n",
    "            '语法上没有明显错误' in row['语病'] or \n",
    "            '没有明显的' in row['语病'] or \n",
    "            '没有显著的' in row['语病'] or\n",
    "            '没有错误' in row['语病'] or\n",
    "            '完全正确' in row['语病'] or\n",
    "            '基本正确' in row['语病'] or    \n",
    "            '基本上正确' in row['语病'] or \n",
    "            '是正确的语法' in row['语病'] or\n",
    "            '标准的英语表达' in row['语病'] or \n",
    "            '语法是正确的' in row['语病'] or\n",
    "            '语法正确' in row['语病'] or\n",
    "            '没有语法错误' in row['语病'] or\n",
    "            '没有。' in row['语病']):\n",
    "            df.loc[i, 'yubing_label'] = 1\n",
    "        else:\n",
    "            df.loc[i, 'yubing_label'] = 0\n",
    "\n",
    "modify_yufa_result(result_tunit_df)\n",
    "modify_yufa_result(result_sen_df)\n",
    "result_tunit_df.to_csv('chatgpt/GPT4/tunit_df_result_mod.csv', index=False, encoding='utf-8-sig')\n",
    "result_sen_df.to_csv('chatgpt/GPT4/sen_df_result_mod.csv', index=False, encoding='utf-8-sig')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################\n",
    "#NOTE 语法 result df processing\n",
    "###############################\n",
    "result_tunit_df = pd.read_csv('chatgpt/yufa/tunit_df_result.csv', encoding='utf-8-sig')\n",
    "result_sen_df = pd.read_csv('chatgpt/yufa/sen_df_result.csv', encoding='utf-8-sig')\n",
    "\n",
    "def modify_yufa_result(df):\n",
    "    for i, row in df.iterrows():\n",
    "        if row['语法错误'] == \"1\":\n",
    "            df.loc[i, 'yufa_label'] = 1\n",
    "        elif row['语法错误'] == \"0\":\n",
    "            df.loc[i, 'yufa_label'] = 0\n",
    "        else:\n",
    "            if ('没有语法错误' in row['语法错误'] or\n",
    "                '没有问题' in row['语法错误'] or\n",
    "                '语法正确' in row['语法错误'] or\n",
    "                '语法上是正确的' in row['语法错误'] or\n",
    "                '语法上没有错误' in row['语法错误'] or\n",
    "                '语法没有错误' in row['语法错误'] or\n",
    "                '没有语法问题' in row['语法错误'] or\n",
    "                '没有明显错误' in row['语法错误'] or\n",
    "                '不算是语法错误' in row['语法错误'] or \n",
    "                '语法上可以说是正确的' in row['语法错误'] or\n",
    "                '语法上没有明显错误' in row['语法错误'] or \n",
    "                '没有明显的' in row['语法错误'] or \n",
    "                '没有显著的' in row['语法错误'] or\n",
    "                '没有错误' in row['语法错误'] or\n",
    "                '完全正确' in row['语法错误'] or\n",
    "                '基本正确' in row['语法错误'] or    \n",
    "                '基本上正确' in row['语法错误'] or \n",
    "                '是正确的语法' in row['语法错误'] or\n",
    "                '标准的英语表达' in row['语法错误'] or \n",
    "                '语法是正确的' in row['语法错误'] or\n",
    "                '语法正确' in row['语法错误']):\n",
    "                df.loc[i, 'yufa_label'] = 1\n",
    "            else:\n",
    "                df.loc[i, 'yufa_label'] = 0\n",
    "\n",
    "modify_yufa_result(result_tunit_df)\n",
    "modify_yufa_result(result_sen_df)\n",
    "result_tunit_df.to_csv('chatgpt/yufa/tunit_df_result_mod.csv', index=False, encoding='utf-8-sig')\n",
    "result_sen_df.to_csv('chatgpt/yufa/sen_df_result_mod.csv', index=False, encoding='utf-8-sig')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################\n",
    "#NOTE 字词 result df processing\n",
    "###############################\n",
    "result_tunit_df = pd.read_csv('chatgpt/zici/tunit_df_result.csv', encoding='utf-8-sig')\n",
    "result_sen_df = pd.read_csv('chatgpt/zici/sen_df_result.csv', encoding='utf-8-sig')\n",
    "\n",
    "def modify_zici_result(df):\n",
    "    for i, row in df.iterrows():\n",
    "        if row['字词错误'] == \"1\":\n",
    "            df.loc[i, 'zici_label'] = 1\n",
    "        elif row['字词错误'] == \"0\":\n",
    "            df.loc[i, 'zici_label'] = 0\n",
    "        else:\n",
    "            if ('没有语法错误' in row['字词错误'] or\n",
    "                '没有问题' in row['字词错误'] or\n",
    "                '语法正确' in row['字词错误'] or\n",
    "                '语法上是正确的' in row['字词错误'] or\n",
    "                '语法上没有错误' in row['字词错误'] or\n",
    "                '语法没有错误' in row['字词错误'] or\n",
    "                '没有语法问题' in row['字词错误'] or\n",
    "                '没有明显错误' in row['字词错误'] or\n",
    "                '不算是语法错误' in row['字词错误'] or \n",
    "                '语法上可以说是正确的' in row['字词错误'] or\n",
    "                '语法上没有明显错误' in row['字词错误'] or \n",
    "                '没有明显的' in row['字词错误'] or \n",
    "                '没有显著的' in row['字词错误'] or\n",
    "                '没有错误' in row['字词错误'] or\n",
    "                '完全正确' in row['字词错误'] or\n",
    "                '基本正确' in row['字词错误'] or    \n",
    "                '基本上正确' in row['字词错误'] or \n",
    "                '是正确的语法' in row['字词错误'] or\n",
    "                '标准的英语表达' in row['字词错误'] or \n",
    "                '正确的中文表达方式' in row['字词错误'] or\n",
    "                '语法是正确的' in row['字词错误'] or\n",
    "                '语法正确' in row['字词错误'] or\n",
    "                '一个字词错误' in row['字词错误'] or\n",
    "                '没有拼写错误' in row['字词错误']):\n",
    "                #NOTE Special case: 没有拼写错误，但是有语法错误\n",
    "                df.loc[i, 'zici_label'] = 1\n",
    "            else:\n",
    "                df.loc[i, 'zici_label'] = 0\n",
    "\n",
    "modify_zici_result(result_tunit_df)\n",
    "modify_zici_result(result_sen_df)\n",
    "result_tunit_df.to_csv('chatgpt/zici/tunit_df_result_mod.csv', index=False, encoding='utf-8-sig')\n",
    "result_sen_df.to_csv('chatgpt/zici/sen_df_result_mod.csv', index=False, encoding='utf-8-sig')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "##########################\n",
    "#NOTE Merge everything!!!\n",
    "##########################\n",
    "yubing_tunit_df = pd.read_csv('chatgpt/yubing/tunit_df_result_mod.csv', encoding='utf-8-sig')\n",
    "yufa_tunit_df = pd.read_csv('chatgpt/yufa/tunit_df_result_mod.csv', encoding='utf-8-sig')\n",
    "zici_tunit_df = pd.read_csv('chatgpt/zici/tunit_df_result_mod.csv', encoding='utf-8-sig')\n",
    "\n",
    "yubing_sen_df = pd.read_csv('chatgpt/yubing/sen_df_result_mod.csv', encoding='utf-8-sig')\n",
    "yufa_sen_df = pd.read_csv('chatgpt/yufa/sen_df_result_mod.csv', encoding='utf-8-sig')\n",
    "zici_sen_df = pd.read_csv('chatgpt/zici/sen_df_result_mod.csv', encoding='utf-8-sig')\n",
    "\n",
    "final_tunit_df = pd.DataFrame({\n",
    "    'sentence': yubing_tunit_df['sentence'],\n",
    "    'ground_truth_label': tunit_df['ground_truth_label'],\n",
    "    'yubing_label': yubing_tunit_df['yubing_label'],\n",
    "    'yubing_content': yubing_tunit_df['语病'],\n",
    "    'yufa_label': yufa_tunit_df['yufa_label'],\n",
    "    'yufa_content': yufa_tunit_df['语法错误'],\n",
    "    'zici_label': zici_tunit_df['zici_label'],\n",
    "    'zici_content': zici_tunit_df['字词错误'],\n",
    "})\n",
    "\n",
    "final_sen_df = pd.DataFrame({\n",
    "    'sentence': yubing_sen_df['sentence'],\n",
    "    'ground_truth_label': sen_df['ground_truth_label'],\n",
    "    'yubing_label': yubing_sen_df['yubing_label'],\n",
    "    'yubing_content': yubing_sen_df['语病'],\n",
    "    'yufa_label': yufa_sen_df['yufa_label'],\n",
    "    'yufa_content': yufa_sen_df['语法错误'],\n",
    "    'zici_label': zici_sen_df['zici_label'],\n",
    "    'zici_content': zici_sen_df['字词错误']\n",
    "})\n",
    "\n",
    "final_tunit_df.to_csv('chatgpt/final_tunit_df.csv', index=True, encoding='utf-8-sig')\n",
    "final_sen_df.to_csv('chatgpt/final_sen_df.csv', index=True, encoding='utf-8-sig')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_tunit_df.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Confustion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_evaluation(golds, predictions, beta=1):\n",
    "    \"\"\"\n",
    "    Prints evaluation statistics comparing golds and predictions, each of which is a sequence of 0/1 labels.\n",
    "    Prints accuracy as well as precision/recall/F1 of the positive class, which can sometimes be informative if either\n",
    "    the golds or predictions are highly biased.\n",
    "\n",
    "    :param golds: gold labels\n",
    "    :param predictions: pred labels\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    num_correct = 0\n",
    "    num_pos_correct = 0\n",
    "    num_pred = 0\n",
    "    num_gold = 0\n",
    "    num_total = 0\n",
    "    if len(golds) != len(predictions):\n",
    "        raise Exception(\"Mismatched gold/pred lengths: %i / %i\" % (len(golds), len(predictions)))\n",
    "    for idx in range(0, len(golds)):\n",
    "        gold = golds[idx]\n",
    "        prediction = predictions[idx]\n",
    "        if prediction == gold:\n",
    "            num_correct += 1\n",
    "        if prediction == 1:\n",
    "            num_pred += 1\n",
    "        if gold == 1:\n",
    "            num_gold += 1\n",
    "        if prediction == 1 and gold == 1:\n",
    "            num_pos_correct += 1\n",
    "        num_total += 1\n",
    "    acc = float(num_correct) / num_total\n",
    "    output_str = \"Accuracy: %i / %i = %f\" % (num_correct, num_total, acc)\n",
    "    prec = float(num_pos_correct) / num_pred if num_pred > 0 else 0.0\n",
    "    rec = float(num_pos_correct) / num_gold if num_gold > 0 else 0.0\n",
    "    f1 = (1 + beta ** 2) * prec * rec / (beta**2 * prec + rec) if prec > 0 and rec > 0 else 0.0\n",
    "    output_str += \";\\nPrecision (fraction of predicted positives that are correct): %i / %i = %f\" % (num_pos_correct, num_pred, prec)\n",
    "    output_str += \";\\nRecall (fraction of true positives predicted correctly): %i / %i = %f\" % (num_pos_correct, num_gold, rec)\n",
    "    output_str += \";\\nF1 (harmonic mean of precision and recall): %f;\\n\" % f1\n",
    "    return output_str\n",
    "\n",
    "def label_compare(label1, label2, golds):\n",
    "    if len(label1) != len(label2):\n",
    "        raise Exception(\"Mismatched gold/pred lengths: %i / %i\" % (len(label1), len(label2)))\n",
    "    total_count = 0\n",
    "    tf_count = 0\n",
    "    ft_count = 0\n",
    "    context_improve_count = 0\n",
    "    unmatch_count = 0\n",
    "    correct_count = 0\n",
    "    for idx in range(0, len(label1)):\n",
    "        if label1[idx] != label2[idx]:\n",
    "            total_count += 1\n",
    "            if label1[idx] == 1:\n",
    "                tf_count += 1\n",
    "            elif label1[idx] == 0:\n",
    "                ft_count += 1\n",
    "        if label1[idx] != label2[idx] and label2[idx] == golds[idx]:\n",
    "            correct_count += 1 \n",
    "        if label1[idx] != golds[idx]:\n",
    "            unmatch_count += 1\n",
    "            if label2[idx] == golds[idx]:\n",
    "                context_improve_count += 1\n",
    "    output_str = \"Number of mismatched labels: %i / %i = %f\\n\" % (total_count, len(label1), float(total_count) / len(label1))\n",
    "    output_str += \"True -> False: %i / %i = %f\\n\" % (tf_count, total_count, float(tf_count) / total_count)\n",
    "    output_str += \"False -> True: %i / %i = %f\\n\" % (ft_count, total_count, float(ft_count) / total_count)\n",
    "    output_str += \"Correct with context: %i / %i = %f\\n\" % (context_improve_count, unmatch_count, float(context_improve_count) / unmatch_count)\n",
    "    output_str += \"Correct count after change: %i\\n\" % (correct_count)\n",
    "    return output_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "result_tunit_df = pd.read_csv('chatgpt/GPT4/tunit_df_result_mod.csv', encoding='utf-8-sig')\n",
    "result_sen_df = pd.read_csv('chatgpt/GPT4/sen_df_result_mod.csv', encoding='utf-8-sig')\n",
    "\n",
    "print(\"## GPT4语病比较 ##\")\n",
    "print(\"------ Tunit Evaluation ------\")\n",
    "print(print_evaluation(result_tunit_df['ground_truth_label'], result_tunit_df['yubing_label']))\n",
    "\n",
    "print(\"------ Sentence Evaluation ------\")\n",
    "print(print_evaluation(result_sen_df['ground_truth_label'], result_sen_df['yubing_label']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "result_tunit_df = pd.read_csv('chatgpt/GPT3.5-turbo/tunit_df_result_mod.csv', encoding='utf-8-sig')\n",
    "result_sen_df = pd.read_csv('chatgpt/GPT3.5-turbo/sen_df_result_mod.csv', encoding='utf-8-sig')\n",
    "\n",
    "print(\"## 语病比较 ##\")\n",
    "print(\"------ Tunit Evaluation ------\")\n",
    "print(\"[No Context]\")\n",
    "print(print_evaluation(result_tunit_df['ground_truth_label'], result_tunit_df['yubing_label']))\n",
    "print(\"[Context]\")\n",
    "print(print_evaluation(result_tunit_df['ground_truth_label'], result_tunit_df['yubing_label_context']))\n",
    "print(\"[Comparison]\")\n",
    "print(label_compare(result_tunit_df['yubing_label'], result_tunit_df['yubing_label_context'], result_tunit_df['ground_truth_label']))\n",
    "\n",
    "print(\"------ Sentence Evaluation ------\")\n",
    "print(print_evaluation(result_sen_df['ground_truth_label'], result_sen_df['yubing_label']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "result_tunit_df = pd.read_csv('chatgpt/final_tunit_df.csv', encoding='utf-8-sig')\n",
    "result_sen_df = pd.read_csv('chatgpt/final_sen_df.csv', encoding='utf-8-sig')\n",
    "print(\"## 语病比较 ##\")\n",
    "print(\"------ Tunit Evaluation ------\")\n",
    "print(print_evaluation(result_tunit_df['ground_truth_label'], result_tunit_df['yubing_label']))\n",
    "print(\"------ Sentence Evaluation ------\")\n",
    "print(print_evaluation(result_sen_df['ground_truth_label'], result_sen_df['yubing_label']))\n",
    "\n",
    "print(\"## 语法比较 ##\")\n",
    "print(\"------ Tunit Evaluation ------\")\n",
    "print(print_evaluation(result_tunit_df['ground_truth_label'], result_tunit_df['yufa_label']))\n",
    "print(\"------ Sentence Evaluation ------\")\n",
    "print(print_evaluation(result_sen_df['ground_truth_label'], result_sen_df['yufa_label']))\n",
    "\n",
    "print(\"## 字词比较 ##\")\n",
    "print(\"------ Tunit Evaluation ------\")\n",
    "print(print_evaluation(result_tunit_df['ground_truth_label'], result_tunit_df['zici_label']))\n",
    "print(\"------ Sentence Evaluation ------\")\n",
    "print(print_evaluation(result_sen_df['ground_truth_label'], result_sen_df['zici_label']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_evaluation(df):\n",
    "    for i, row in result_tunit_df.iterrows():\n",
    "        if row['yufa_label'] == 1 and row['zici_label'] == 1:\n",
    "            df.loc[i, 'combine_label'] = 1\n",
    "        else:\n",
    "            df.loc[i, 'combine_label'] = 0\n",
    "\n",
    "combine_evaluation(result_tunit_df)\n",
    "combine_evaluation(result_sen_df)\n",
    "\n",
    "print(print_evaluation(result_tunit_df['yubing_label'], result_tunit_df['combine_label']))\n",
    "print(print_evaluation(result_sen_df['yubing_label'], result_sen_df['combine_label']))\n",
    "print(print_evaluation(result_tunit_df['ground_truth_label'], result_tunit_df['combine_label']))\n",
    "print(print_evaluation(result_sen_df['ground_truth_label'], result_sen_df['combine_label']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = range(11) \n",
    "file_name_pre = 'chatgpt/yubing/tunit_df_result'\n",
    "\n",
    "def overwrite(file_name_pre, index):\n",
    "    # index [0, 1, 2 ... 10]\n",
    "    for i in index:\n",
    "        file_name = file_name_pre + str(i) + '.csv'\n",
    "        if i == 0:\n",
    "            df = pd.read_csv(file_name, encoding='utf-8-sig')\n",
    "        else:\n",
    "            df_new = pd.read_csv(file_name, encoding='utf-8-sig')\n",
    "            for i, row in df_new.iterrows():\n",
    "                if df_new[i]['result'] is not None:\n",
    "                    df[i]['result'] = df_new[i]['result']\n",
    "    df.to_csv(file_name, index=False, encoding='utf-8-sig')\n",
    "\n",
    "# hard append...?\n",
    "overwrite(file_name_pre, index)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "75273f6ed8af91899ebc591bf6ae2fd0716c5db2515d7097a000123632f4e53a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
